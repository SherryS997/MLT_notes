<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MLT Notes - Supervised Learning - Regression - Least Squares; Bayesian view</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Supervised Learning - Regression - Least Squares; Bayesian view</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">MLT Notes</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Home</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Content</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../about.html" class="sidebar-item-text sidebar-link">About</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Notes</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk01.html" class="sidebar-item-text sidebar-link">Week-01</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk02.html" class="sidebar-item-text sidebar-link">Week-02</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk03.html" class="sidebar-item-text sidebar-link">Week-03</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk04.html" class="sidebar-item-text sidebar-link">Week-04</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk05.html" class="sidebar-item-text sidebar-link active">Week-05</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk06.html" class="sidebar-item-text sidebar-link">Week-06</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk07.html" class="sidebar-item-text sidebar-link">Week-07</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-supervised-learning" id="toc-introduction-to-supervised-learning" class="nav-link active" data-scroll-target="#introduction-to-supervised-learning">Introduction to Supervised Learning</a></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a></li>
  <li><a href="#optimizing-the-error-function" id="toc-optimizing-the-error-function" class="nav-link" data-scroll-target="#optimizing-the-error-function">Optimizing the Error Function</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#stochastic-gradient-descent" id="toc-stochastic-gradient-descent" class="nav-link" data-scroll-target="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
  </ul></li>
  <li><a href="#kernel-regression" id="toc-kernel-regression" class="nav-link" data-scroll-target="#kernel-regression">Kernel Regression</a></li>
  <li><a href="#probabilistic-view-of-linear-regression" id="toc-probabilistic-view-of-linear-regression" class="nav-link" data-scroll-target="#probabilistic-view-of-linear-regression">Probabilistic View of Linear Regression</a></li>
  <li><a href="#credits" id="toc-credits" class="nav-link" data-scroll-target="#credits">Credits</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Supervised Learning - Regression - Least Squares; Bayesian view</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction-to-supervised-learning" class="level1">
<h1>Introduction to Supervised Learning</h1>
<p>Supervised learning is a machine learning technique in which the algorithm is trained on labeled data, where the target variable or the outcome is known. The goal of supervised learning is to learn a mapping between the input data and the corresponding output variable.</p>
<p>Given a dataset <span class="math inline">\(\{x_1, \ldots, x_n\}\)</span> where <span class="math inline">\(x_i \in \mathbb{R}^d\)</span>, let <span class="math inline">\(\{y_1, \ldots, y_n\}\)</span> be the labels, where <span class="math inline">\(y_i\)</span> could belong to the following:</p>
<ul>
<li>Regression: <span class="math inline">\(y_i \in \mathbb{R}\)</span> Example: Rainfall Prediction</li>
<li>Binary Classification: <span class="math inline">\(y_i \in \{0, 1\}\)</span> Example: Distinguishing between cats and dogs</li>
<li>Multi-class Classification: <span class="math inline">\(y_i \in \{0, 1, \ldots, K\}\)</span> Example: Digit classification</li>
</ul>
</section>
<section id="linear-regression" class="level1">
<h1>Linear Regression</h1>
<p>Linear regression is a supervised learning algorithm used to predict a continuous output variable based on one or more input features, assuming a linear relationship between the input and output variables. The goal of linear regression is to find the line of best fit that minimizes the sum of squared errors between the predicted and actual output values.</p>
<p>Given a dataset <span class="math inline">\(\{x_1, \ldots, x_n\}\)</span> where <span class="math inline">\(x_i \in \mathbb{R}^d\)</span>, let <span class="math inline">\(\{y_1, \ldots, y_n\}\)</span> be the labels, where <span class="math inline">\(y_i \in \mathbb{R}\)</span>.</p>
<p>The goal of linear regression is to find the mapping between input and output variables. i.e. <span class="math display">\[
h: \mathbb{R}^d \rightarrow \mathbb{R}
\]</span> The error for the above mapping function is given by, <span class="math display">\[
\text{error}(h) = \sum _{i=1} ^n (h(x_i) - y_i) ^2)
\]</span> This error can be as small as zero, and this is achieved when <span class="math inline">\(h(x_i)=y_i \hspace{0.5em} \forall i\)</span>. But this may not be the desired output as it may represent nothing more than memorizing the data and its outputs.</p>
<p>The memorization problem can be mitigated by introducing a structure to the mapping. The simplest structure being of the linear kind, we shall use it as the underlying structure for our data.</p>
<p>Let <span class="math inline">\(\mathcal{H}_{\text{linear}}\)</span> represent the solution space for the mapping in the linear space. <span class="math display">\[
\mathcal{H}_{\text{linear}}=\Biggr \lbrace{h_w: \mathbb{R}^d \rightarrow \mathbb{R} \hspace{0.5em} s.t. \hspace{0.5em} h_w(x)=w^Tx \hspace{0.5em} \forall w \in \mathbb{R}^d } \Biggr \rbrace
\]</span> Therefore, our goal is, <span class="math display">\[\begin{align*}
\min _{h \in \mathcal{H}_{\text{linear}}} \sum _{i=1} ^n (h(x_i) - y_i) ^2 \\
\text{Equivalently} \\
\min _{w \in \mathbb{R}^d} \sum _{i=1} ^n (w^Tx_i - y_i) ^2
\end{align*}\]</span> Optimizing the above is the entire point of the linear regression algorithm.</p>
</section>
<section id="optimizing-the-error-function" class="level1">
<h1>Optimizing the Error Function</h1>
<p>The minimization equation can be rewritten in the vectorized form as, <span class="math display">\[
\min _{w \in \mathbb{R}^d} || X^Tw - y || ^2 _2
\]</span> Let this be a function of <span class="math inline">\(w\)</span> and as follows: <span class="math display">\[\begin{align*}
f(w) &amp;= \min _{w \in \mathbb{R}^d} || X^Tw - y || ^2 _2 \\
f(w) &amp;= (X^Tw - y)^T(X^Tw - y) \\
\therefore\bigtriangledown f(w) &amp;= 2(XX^T)w - 2(Xy) \\
\end{align*}\]</span> Setting the above equation to zero, we get <span class="math display">\[\begin{align*}
(XX^T)w^* &amp;= Xy \\
\therefore w^* &amp;= (XX^T)^+Xy
\end{align*}\]</span> where <span class="math inline">\((XX^T)^+\)</span> is the pseudo-inverse of <span class="math inline">\(XX^T\)</span>.</p>
<p>On further analysis of the above equation, we can say that <span class="math inline">\(X^Tw\)</span> is the projection of the labels onto the subspace spanned by the features.</p>
</section>
<section id="gradient-descent" class="level1">
<h1>Gradient Descent</h1>
<p>The normal equation for linear regression, as seen before, needs the calculation of <span class="math inline">\((XX^T)^+\)</span> which can computationally be of <span class="math inline">\(O(d^3)\)</span> complexity.</p>
<p>As we know <span class="math inline">\(w^*\)</span> is the solution of an unconstrained optimization problem, we can solve it using gradient descent. It is given by, <span class="math display">\[\begin{align*}
w^{t+1} &amp;= w^t - \eta^t \bigtriangledown f(w^t) \\
\therefore w^{t+1} &amp;= w^t - \eta^t \left [ 2(XX^T)w - 2(Xy) \right ]
\end{align*}\]</span> where <span class="math inline">\(\eta\)</span> is a scalar used to control the step-size of the descent and <span class="math inline">\(t\)</span> is the current iteration.</p>
<p>Even in the above equation, the calculation of <span class="math inline">\(XX^T\)</span> is needed which is a computational expensive task. Is there a way to adapt this further?</p>
<section id="stochastic-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
<p>Stochastic gradient descent (SGD) is an optimization algorithm used in machine learning for finding the parameters that minimize the loss function of a model. In contrast to traditional gradient descent, which updates the model parameters based on the entire dataset, SGD updates the parameters based on a randomly selected subset of the data, known as a batch. This results in faster training times and makes SGD particularly useful for large datasets.</p>
<p>For every step <span class="math inline">\(t\)</span>, rather than updating <span class="math inline">\(w\)</span> using the entire dataset, we use a small randomly selected (<span class="math inline">\(k\)</span>) data points to update <span class="math inline">\(w\)</span>. Therefore, the new gradient is <span class="math inline">\(2(\tilde{X}\tilde{X}^Tw^t - \tilde{X}\tilde{y})\)</span> where <span class="math inline">\(\tilde{X}\)</span> and <span class="math inline">\(\tilde{y}\)</span> is the small sample randomly selected from the dataset. This is manageable because <span class="math inline">\(\tilde{X} \in \mathbb{R}^{d \times k}\)</span> which is considerably smaller that <span class="math inline">\(X\)</span>.</p>
<p>After T rounds, we use, <span class="math display">\[
w ^T _{SGD} = \frac{1}{T}  \sum _{i=1} ^T w^i
\]</span> This has a certain guarantee to have optimal convergence partly because of the randomness involved in it.</p>
</section>
</section>
<section id="kernel-regression" class="level1">
<h1>Kernel Regression</h1>
<p>What if the data points lie in a non-linear sub-space? Just like in the case of clustering non-linear data, we will use kernel functions here too.</p>
<p>Let <span class="math inline">\(w^* = X\alpha^*\)</span> for some <span class="math inline">\(\alpha^* \in \mathbb{R}^d\)</span>. <span class="math display">\[\begin{align*}
X\alpha^*&amp;=w^*\\
\therefore X\alpha^* &amp;=(XX^T)^+Xy \\
(XX^T)X\alpha^* &amp;=(XX^T)(XX^T)^+Xy \\
(XX^T)X\alpha^* &amp;=Xy \\
X^T(XX^T)X\alpha^* &amp;=X^TXy \\
(X^TX)^2\alpha^* &amp;=X^TXy \\
K^2\alpha^* &amp;=Ky \\
\therefore \alpha^* &amp;=K^{-1}y
\end{align*}\]</span> where <span class="math inline">\(K \in \mathbb{R}^{n \times n}\)</span> and <span class="math inline">\(K\)</span> can be obtained using a kernel function like the Polynomial Kernel or RBF Kernel.</p>
<p>How to predict using alpha and the kernel function? Let <span class="math inline">\(X_{test} \in R^{d \times m}\)</span> be the test dataset. We predict by, <span class="math display">\[\begin{align*}
w^*\phi(X_{test}) &amp;=  \sum _{i=1} ^n \alpha_i^* k(x_i, x_{test_i})
\end{align*}\]</span> where <span class="math inline">\(\alpha_i^*\)</span> gives the importance of the <span class="math inline">\(i^{th}\)</span> datapoint towards <span class="math inline">\(w^*\)</span> and <span class="math inline">\(k(x_i, x_{test_i})\)</span> shows how similar <span class="math inline">\(x_{test_i}\)</span> is to <span class="math inline">\(x_i\)</span>.</p>
</section>
<section id="probabilistic-view-of-linear-regression" class="level1">
<h1>Probabilistic View of Linear Regression</h1>
<p>Given a dataset <span class="math inline">\(\{x_1, \ldots, x_n\}\)</span> where <span class="math inline">\(x_i \in \mathbb{R}^d\)</span>, let <span class="math inline">\(\{y_1, \ldots, y_n\}\)</span> be the labels, where <span class="math inline">\(y_i \in \mathbb{R}\)</span>. The probability of <span class="math inline">\(y_i\)</span> given <span class="math inline">\(x_i\)</span> is given by, <span class="math display">\[
P(y|X) \sim w^TX + \epsilon
\]</span> where <span class="math inline">\(w \in R^d\)</span> is an unknown but fixed value and <span class="math inline">\(\epsilon\)</span> is the noise corresponding to distribution <span class="math inline">\(\mathcal{N}(0, \sigma^2)\)</span>.</p>
<p>Because of the probabilistic nature of this problem, this can be viewed as an estimation problem and the best solution can be approached using Maximum Likelihood. This is given by, <span class="math display">\[\begin{align*}
\mathcal{L}\left(w; \hspace{0.5em} \begin{array}{cccc}
x_1, x_2, \ldots, x_n \\
y_1, y_2, \ldots, y_n\\
\end{array} \right )
&amp;= \prod _{i=1} ^n \left [ \frac{1}{\sqrt{2\pi}\sigma}  e^{\frac{-(w^Tx_i - y_i)^2}{2\sigma^2}} \right ] \\
\log\mathcal{L}\left(w; \hspace{0.5em} \begin{array}{cccc}
x_1, x_2, \ldots, x_n \\
y_1, y_2, \ldots, y_n\\
\end{array} \right )
&amp;= \sum _{i=1} ^n \frac{-(w^Tx_i - y_i)^2}{2\sigma^2} \frac{1}{\sqrt{2\pi}\sigma}
\end{align*}\]</span> <span class="math display">\[
\text{Taking gradients w.r.t. } w \text{ on both sides}
\]</span> <span class="math display">\[
\hat{w}_{ML} = w^* = (XX^T)^+Xy
\]</span> Therefore, Maximum Likelihood Estimator assuming Zero mean Gaussian Noise is the same as linear regression with square error.</p>
</section>
<section id="credits" class="level1">
<h1>Credits</h1>
<p>Professor Arun Rajkumar: The content as well as the notations are from his slides and lecture.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>