<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MLT Notes - Discriminative Models - Perceptron; Logistic Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Discriminative Models - Perceptron; Logistic Regression</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">MLT Notes</a> 
        <div class="sidebar-tools-main">
    <a href="https://www.linkedin.com/in/sherry-thomassp/" title="" class="sidebar-tool px-1"><i class="bi bi-linkedin"></i></a>
    <a href="https://github.com/SherryS997/MLT_notes" title="" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Home</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Content</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../about.html" class="sidebar-item-text sidebar-link">About</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Notes</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk01.html" class="sidebar-item-text sidebar-link">Week-01</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk02.html" class="sidebar-item-text sidebar-link">Week-02</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk03.html" class="sidebar-item-text sidebar-link">Week-03</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk04.html" class="sidebar-item-text sidebar-link">Week-04</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk05.html" class="sidebar-item-text sidebar-link">Week-05</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk06.html" class="sidebar-item-text sidebar-link">Week-06</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk07.html" class="sidebar-item-text sidebar-link">Week-07</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk08.html" class="sidebar-item-text sidebar-link">Week-08</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk09.html" class="sidebar-item-text sidebar-link active">Week-09</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#perceptron-learning-algorithm" id="toc-perceptron-learning-algorithm" class="nav-link active" data-scroll-target="#perceptron-learning-algorithm">Perceptron Learning Algorithm</a>
  <ul class="collapse">
  <li><a href="#analysis-of-the-update-rule" id="toc-analysis-of-the-update-rule" class="nav-link" data-scroll-target="#analysis-of-the-update-rule">Analysis of the Update Rule</a></li>
  <li><a href="#further-assumptions" id="toc-further-assumptions" class="nav-link" data-scroll-target="#further-assumptions">Further Assumptions</a></li>
  </ul></li>
  <li><a href="#proof-of-convergence-of-perceptron-algorithm" id="toc-proof-of-convergence-of-perceptron-algorithm" class="nav-link" data-scroll-target="#proof-of-convergence-of-perceptron-algorithm">Proof of Convergence of Perceptron Algorithm</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#sigmoid-function" id="toc-sigmoid-function" class="nav-link" data-scroll-target="#sigmoid-function">Sigmoid Function</a></li>
  <li><a href="#logistic-regression-1" id="toc-logistic-regression-1" class="nav-link" data-scroll-target="#logistic-regression-1">Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#kernel-and-regularized-versions" id="toc-kernel-and-regularized-versions" class="nav-link" data-scroll-target="#kernel-and-regularized-versions">Kernel and Regularized Versions</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#credits" id="toc-credits" class="nav-link" data-scroll-target="#credits">Credits</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Discriminative Models - Perceptron; Logistic Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>PDF Link: <a href="../notes/Wk09.pdf">notes</a></p>
<section id="perceptron-learning-algorithm" class="level1">
<h1>Perceptron Learning Algorithm</h1>
<p>The <strong>Perceptron Learning Algorithm</strong> is a type of supervised learning algorithm used for binary classification tasks. It involves iteratively adjusting the weights of a linear combination of input features until a decision boundary that separates the two classes is found. The algorithm is a type of discriminative classification as it directly models the boundary between classes rather than modeling the underlying probability distribution of each class.</p>
<p>Let <span class="math inline">\(D=\{(x_1, y_1), \ldots, (x_n,y_n)\}\)</span> be the dataset, where <span class="math inline">\(x_i \in \mathbb{R}^d\)</span> and <span class="math inline">\(y_i \in \{0, 1\}\)</span>.</p>
<p>Assumptions made in the algorithm:</p>
<ol type="1">
<li><span class="math inline">\(P(y=1|x) = 1\)</span> if <span class="math inline">\(w^Tx\ge0\)</span>, <span class="math inline">\(0\)</span> otherwise.</li>
<li><strong>Linear Separability Assumption</strong>: The Linear Separability Assumption is the assumption made in some machine learning algorithms, including the Perceptron Learning Algorithm, that the classes to be classified can be separated by a linear decision boundary. This means that there exists a hyperplane in the feature space that can separate the data points of the two classes.</li>
</ol>
<p>The objective function is given by, <span class="math display">\[
\min _{h \in \mathcal{H}} \sum _{i=1} ^n \mathbb{1}\left ( h(x_i) \ne y_i \right )
\]</span> In general, this is an NP-Hard Problem even if <span class="math inline">\(\mathcal{H}\)</span> only accounts for the Linear Hypotheses.</p>
<p>Under the Linear Separability Assumption, let <span class="math inline">\(\exists w \in \mathbb{R}^d\)</span> s.t. <span class="math inline">\(\text{sign}(w^Tx_i)=y_i\)</span> <span class="math inline">\(\forall i \in [n]\)</span>.</p>
<p>We solve this problem of convergence using an iterative algorithm. The algorithm is as follows:</p>
<ul>
<li>Assign <span class="math inline">\(w^0 = 0 \in \mathbb{R}^d\)</span></li>
<li>Until Convergence:
<ul>
<li>Pick <span class="math inline">\((x_i, y_i)\)</span> pair from the dataset</li>
<li>if <span class="math inline">\(\text{sign}(w^Tx_i)==y_i\)</span>
<ul>
<li>do nothing</li>
</ul></li>
<li>else
<ul>
<li><span class="math inline">\(w^{(t+1)} = w^t + x_iy_i \hspace{2em} \leftarrow\)</span> update rule.</li>
</ul></li>
<li>end</li>
</ul></li>
</ul>
<section id="analysis-of-the-update-rule" class="level2">
<h2 class="anchored" data-anchor-id="analysis-of-the-update-rule">Analysis of the Update Rule</h2>
<p>For a training example <span class="math inline">\((x, y)\)</span>, where <span class="math inline">\(x\)</span> is the input and <span class="math inline">\(y\)</span> is the correct output (either <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span>), the perceptron algorithm updates its weight vector <span class="math inline">\(w\)</span> as follows:</p>
<ul>
<li>If the prediction of the perceptron on <span class="math inline">\(x\)</span> is correct (i.e., <span class="math inline">\(\text{sign}(w^Tx_i)==y_i\)</span>), then no update is performed.</li>
<li>If the prediction of the perceptron on <span class="math inline">\(x\)</span> is incorrect (i.e., <span class="math inline">\(\text{sign}(w^Tx_i)\ne y_i\)</span>), then the weights are updated by adding the product of the input vector and the correct output to the current weight vector: <span class="math inline">\(w^{(t+1)} = w^t + x_iy_i\)</span>.</li>
</ul>
<p>This update rule effectively moves the decision boundary in the direction of the correct classification for the misclassified example. It is guaranteed to converge to a linearly separable solution if the data is linearly separable. However, if the data is not linearly separable, the perceptron algorithm may not converge to a solution.</p>
</section>
<section id="further-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="further-assumptions">Further Assumptions</h2>
<p>We make three further assumptions:</p>
<ol type="1">
<li><strong>Linear Separability with <span class="math inline">\(\gamma\)</span>-Margin</strong>: A dataset <span class="math inline">\(D=\{(x_1, y_1), \ldots, (x_n,y_n)\}\)</span> is linearly separable with <span class="math inline">\(\gamma\)</span>-margin if <span class="math inline">\(\exists w^* \in \mathbb{R}^d\)</span> s.t. <span class="math inline">\((w^{*T}x_i)y_i\ge\gamma\)</span> <span class="math inline">\(\forall i\)</span> for some <span class="math inline">\(\gamma&gt;0\)</span>.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/Linear Separability with gamma-Margin.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Linear Separability with <span class="math inline">\(\gamma\)</span>-Margin</figcaption><p></p>
</figure>
</div>
<ol start="2" type="1">
<li><strong>Radius Assumption</strong>: Let some <span class="math inline">\(R&gt;0 \in \mathbb{R}\)</span>, <span class="math inline">\(\forall i \in D\)</span> <span class="math inline">\(||x_i||\le R\)</span>. In short, let <span class="math inline">\(R\)</span> be the length of the datapoint furthest from the center.</li>
<li><strong>Normal Length for <span class="math inline">\(w^*\)</span></strong>: Let <span class="math inline">\(w^*\)</span> be of unit length.</li>
</ol>
</section>
</section>
<section id="proof-of-convergence-of-perceptron-algorithm" class="level1">
<h1>Proof of Convergence of Perceptron Algorithm</h1>
<p>Let <span class="math inline">\(l\)</span> denote the current mistake number. Using everything upto now, we can see, <span class="math display">\[\begin{align*}
w^{l+1} &amp;= w^l + xy \\
||w^{l+1}||^2&amp;=||w^l + xy||^2 \\
&amp;=(w^l + xy)^T(w^l + xy) \\
&amp;=||w^l||^2 + 2(w^{lT}x)y+ ||x||^2y^2 \\
\therefore ||w^{l+1}||^2&amp;\le ||w^l||^2 + R^2 \\
&amp;\le (||w^{l-1}||^2 + R^2)  + R^2 \\
&amp;\le ||w^0||^2 + lR^2 \\
\therefore ||w^{l+1}||^2&amp;\le lR^2 \hspace{6em} \ldots[1]
\end{align*}\]</span> We can also see, <span class="math display">\[\begin{align*}
w^{l+1} &amp;= w^l + xy \\
(w^{l+1})^Tw^* &amp;= (w^l + xy)^Tw^* \\
&amp;= w^{lT}w^* + (w^{*T}x)y \\
\therefore (w^{l+1})^Tw^* &amp;\ge w^{lT}w^* + \gamma \\
&amp;\ge (w^{l-1T}w^* + \gamma) + \gamma \\
&amp;\ge w^{0T}w^* + l\gamma \\
\therefore (w^{l+1})^Tw^* &amp;\ge l\gamma \\
((w^{l+1})^Tw^*)^2 &amp;\ge l^2\gamma^2 \\
||w^{l+1}||^2||w^*||^2 &amp;\ge l^2\gamma^2 \hspace{0.5em}\ldots\text{Using Cauchy-Schwartz}\\
\therefore ||w^{l+1}||^2 &amp;\ge l^2\gamma^2 \hspace{6em} \ldots[2]
\end{align*}\]</span> Therefore, from <span class="math inline">\([1]\)</span> and <span class="math inline">\([2]\)</span>, we get, <span class="math display">\[\begin{align*}
l^2\gamma^2 &amp;\le ||w^{l+1}||^2 \le lR^2 \\
l^2\gamma^2 &amp;\le  lR^2 \\
\therefore l &amp;\le \frac{R^2}{\gamma^2}
\end{align*}\]</span> Hence, from the above equation, we can conclude that for a dataset that follows Linear Separability with <span class="math inline">\(\gamma\)</span>-Margin, and has a finite Radius <span class="math inline">\(R\)</span>, the perceptron algorithm has an upper bound for the number of mistakes.</p>
<p>Therefore, perceptron converges.</p>
</section>
<section id="logistic-regression" class="level1">
<h1>Logistic Regression</h1>
<section id="sigmoid-function" class="level2">
<h2 class="anchored" data-anchor-id="sigmoid-function">Sigmoid Function</h2>
<p>Until now, we used the <span class="math inline">\(\text{sign}\)</span> function to get the class for the output. But can we also provide the probabilities for these outputs?</p>
<p>Let <span class="math inline">\(z=w^Tx\)</span> and <span class="math inline">\(z \in \mathbb{R}\)</span>. How can we map <span class="math inline">\([-\infty, \infty]\rightarrow[0,1]\)</span>? For this, we use the <strong>Sigmoid Function</strong>. It is given by, <span class="math display">\[
g(z) = \frac{1}{1+e^{-z}}
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/sigmoid-function.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Sigmoid Function</figcaption><p></p>
</figure>
</div>
<p>The sigmoid function is often used in machine learning as an activation function for neural networks. It has a characteristic S-shaped curve, which makes it useful for modeling processes that have a threshold or saturation point, such as logistic growth or binary classification problems.</p>
<p>When the input value is large and positive, the sigmoid function output approaches 1, and when the input value is large and negative, the sigmoid function output approaches 0. When the input value is 0, the sigmoid function output is exactly 0.5.</p>
<p>The term “sigmoid” comes from the Greek word “sigmoides,” which means “shaped like the letter sigma” (<span class="math inline">\(\Sigma\)</span>). The letter sigma has a similar shape to the sigmoid function’s characteristic S-shaped curve, which is likely the reason for the function’s name.</p>
</section>
<section id="logistic-regression-1" class="level2">
<h2 class="anchored" data-anchor-id="logistic-regression-1">Logistic Regression</h2>
<p>Logistic regression is a statistical method used to analyze and model the relationship between a binary (two-valued) dependent variable and one or more independent variables, which can be either continuous or categorical. The goal of logistic regression is to estimate the probability that the dependent variable is one of the two possible values, given the values of the independent variables.</p>
<p>In logistic regression, the dependent variable is modeled as a function of the independent variables using a logistic(sigmoid) function, which produces an S-shaped curve that ranges between 0 and 1. The logistic function transforms the output of a linear combination of the independent variables into a probability estimate, which can then be used to classify new observations.</p>
<p>Let <span class="math inline">\(D=\{(x_1, y_1), \ldots, (x_n,y_n)\}\)</span> be the dataset, where <span class="math inline">\(x_i \in \mathbb{R}^d\)</span> and <span class="math inline">\(y_i \in \{0, 1\}\)</span>.</p>
<p>We know, <span class="math display">\[
P(y=1|x) = g(w^Tx_i) = \frac{1}{1+e^{-w^Tx}}
\]</span> Using Maximum Likelihood, we get <span class="math display">\[\begin{align*}
\mathcal{L}(w;\text{Data}) &amp;= \prod _{i=1} ^{n} (g(w^Tx_i))^{y_i}(1- g(w^Tx_i))^{1-y_i} \\
\log(\mathcal{L}(w;\text{Data})) &amp;= \sum _{i=1} ^{n} y_i\log(g(w^Tx_i))+(1-y_i)\log(1- g(w^Tx_i)) \\
&amp;= \sum _{i=1} ^{n} y_i\log\left(\frac{1}{1+e^{-w^Tx_i}}\right)+(1-y_i)\log\left(\frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}}\right) \\
&amp;= \sum _{i=1} ^{n} \left [ (1-y_i)(-w^Tx_i) - \log(1+e^{-w^Tx_i}) \right ]
\end{align*}\]</span> Therefore, our objective, which is maximizing the log-likelihood function, is given by, <span class="math display">\[
\max _{w}\sum _{i=1} ^{n} \left [ (1-y_i)(-w^Tx_i) - \log(1+e^{-w^Tx_i}) \right ]
\]</span> But, there is no closed form solution for this. And hence, we use gradient descent for convergence.</p>
<p>The gradient is given by, <span class="math display">\[\begin{align*}
\nabla \log(\mathcal{L}(w;\text{Data})) &amp;= \sum _{i=1} ^{n} \left [ (1-y_i)(-x_i) - \left( \frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \right ) (-x_i) \right ] \\
&amp;= \sum _{i=1} ^{n} \left [ -x_i + x_iy_i + x_i \left( \frac{e^{-w^Tx_i}}{1+e^{-w^Tx_i}} \right ) \right ] \\
&amp;= \sum _{i=1} ^{n} \left [ x_iy_i - x_i \left( \frac{1}{1+e^{-w^Tx_i}} \right ) \right ] \\
\nabla \log(\mathcal{L}(w;\text{Data})) &amp;= \sum _{i=1} ^{n} \left [ x_i \left(y_i - \frac{1}{1+e^{-w^Tx_i}} \right ) \right ]
\end{align*}\]</span> Using the Gradient Descent update rule, we get, <span class="math display">\[\begin{align*}
w_{t+1} &amp;= w_t + \eta_t\nabla \log(\mathcal{L}(w;\text{Data})) \\
&amp;= w_t + \eta_t  \left ( \sum _{i=1} ^{n} x_i \left(y_i - \frac{1}{1+e^{-w^Tx_i}} \right ) \right )
\end{align*}\]</span></p>
<section id="kernel-and-regularized-versions" class="level3">
<h3 class="anchored" data-anchor-id="kernel-and-regularized-versions">Kernel and Regularized Versions</h3>
<p>We can argue that <span class="math inline">\(w^*=\displaystyle\sum _{i=1} ^{n}\alpha_ix_i\)</span>, and therefore, can be Kernelized. For further details, refer to this <a href="https://cs229.stanford.edu/extra-notes/representer-function.pdf#section.2">link.</a></p>
<p>The regularized version is given by, <span class="math display">\[
\min _{w}\sum _{i=1} ^{n} \left [ \log(1+e^{-w^Tx_i}) + w^Tx_i(1-y_i) \right ] + \frac{\lambda}{2}||w||^2
\]</span> where <span class="math inline">\(\frac{\lambda}{2}||w||^2\)</span> is the regualizer and <span class="math inline">\(\lambda\)</span> is found using cross-validation.</p>
</section>
</section>
</section>
<section id="credits" class="level1">
<h1>Credits</h1>
<p>Professor Arun Rajkumar: The content as well as the notations are from his slides and lecture.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>