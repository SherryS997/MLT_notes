<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MLT Notes - Introduction; Unsupervised Learning - Representation learning - PCA</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Introduction; Unsupervised Learning - Representation learning - PCA</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">MLT Notes</a> 
        <div class="sidebar-tools-main">
    <a href="https://www.linkedin.com/in/sherry-thomassp/" title="" class="sidebar-tool px-1"><i class="bi bi-linkedin"></i></a>
    <a href="https://github.com/SherryS997/MLT_notes" title="" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Home</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Content</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../about.html" class="sidebar-item-text sidebar-link">About</a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Notes</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk01.html" class="sidebar-item-text sidebar-link active">Week-01</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk02.html" class="sidebar-item-text sidebar-link">Week-02</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk03.html" class="sidebar-item-text sidebar-link">Week-03</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk04.html" class="sidebar-item-text sidebar-link">Week-04</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk05.html" class="sidebar-item-text sidebar-link">Week-05</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk06.html" class="sidebar-item-text sidebar-link">Week-06</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk07.html" class="sidebar-item-text sidebar-link">Week-07</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk08.html" class="sidebar-item-text sidebar-link">Week-08</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/Wk09.html" class="sidebar-item-text sidebar-link">Week-09</a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-machine-learning" id="toc-introduction-to-machine-learning" class="nav-link active" data-scroll-target="#introduction-to-machine-learning">Introduction to Machine Learning</a>
  <ul class="collapse">
  <li><a href="#broad-paradigms-of-machine-learning" id="toc-broad-paradigms-of-machine-learning" class="nav-link" data-scroll-target="#broad-paradigms-of-machine-learning">Broad Paradigms of Machine Learning</a></li>
  </ul></li>
  <li><a href="#representation-learning" id="toc-representation-learning" class="nav-link" data-scroll-target="#representation-learning">Representation Learning</a>
  <ul class="collapse">
  <li><a href="#potential-algorithm" id="toc-potential-algorithm" class="nav-link" data-scroll-target="#potential-algorithm">Potential Algorithm</a></li>
  </ul></li>
  <li><a href="#principal-component-analysis" id="toc-principal-component-analysis" class="nav-link" data-scroll-target="#principal-component-analysis">Principal Component Analysis</a>
  <ul class="collapse">
  <li><a href="#approximate-representation" id="toc-approximate-representation" class="nav-link" data-scroll-target="#approximate-representation">Approximate Representation</a></li>
  <li><a href="#p.c.a.-algorithm" id="toc-p.c.a.-algorithm" class="nav-link" data-scroll-target="#p.c.a.-algorithm">P.C.A. Algorithm</a></li>
  </ul></li>
  <li><a href="#credits" id="toc-credits" class="nav-link" data-scroll-target="#credits">Credits</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">Introduction; Unsupervised Learning - Representation learning - PCA</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>PDF Link: <a href="../notes/Wk01.pdf">notes</a></p>
<section id="introduction-to-machine-learning" class="level1">
<h1>Introduction to Machine Learning</h1>
<p>Machine Learning is a sub-field of artificial intelligence concerned with the design of algorithms and statistical models that allow computers to learn from and make predictions or decisions based on data, without being explicitly programmed. It utilizes mathematical optimization, algorithms, and computational models to analyze and understand patterns in data and make predictions about future outcomes.</p>
<p>It can be further explained as follows:</p>
<ul>
<li>Why: Machine Learning is used to automate tasks that would otherwise require human intelligence, to process vast amounts of data, and to make predictions or decisions with greater accuracy than traditional approaches. It also has surged in popularity in recent years.</li>
<li>Where: Machine Learning is applied in various fields such as computer vision, natural language processing, finance, and healthcare, among others.Where: Machine Learning is applied in various fields such as computer vision, natural language processing, finance, and healthcare, among others.</li>
<li>What: Machine Learning departs from traditional procedural approaches, instead it is driven by data analysis. Rather than memorizing specific examples, it seeks to generalize patterns in the data. Machine Learning is not based on magic, rather it relies on mathematical principles and algorithms.</li>
</ul>
<section id="broad-paradigms-of-machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="broad-paradigms-of-machine-learning">Broad Paradigms of Machine Learning</h2>
<ol type="1">
<li><strong>Supervised Learning</strong>:Supervised Machine Learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the data includes both inputs and their corresponding outputs. The goal of supervised learning is to build a model that can accurately predict the output for new, unseen input data. Few examples:</li>
</ol>
<ul>
<li>Linear regression for predicting a continuous output</li>
<li>Logistic regression for binary classification problems</li>
<li>Decision trees for non-linear classification and regression problems</li>
<li>Support Vector Machines for binary and multi-class classification problems</li>
<li>Neural Networks for complex non-linear problems in various domains such as computer vision, natural language processing, and speech recognition</li>
</ul>
<ol start="2" type="1">
<li><strong>Unsupervised Learning</strong>: Unsupervised Machine Learning is a type of machine learning where the algorithm is trained on an unlabeled dataset, meaning that only the inputs are provided and no corresponding outputs. The goal of unsupervised learning is to uncover patterns or relationships within the data without any prior knowledge or guidance. Few examples:</li>
</ol>
<ul>
<li>Clustering algorithms such as K-means, hierarchical clustering, and density-based clustering, used to group similar data points together into clusters</li>
<li>Dimensionality reduction techniques such as Principal Component Analysis (PCA), used to reduce the number of features in a dataset while preserving the maximum amount of information</li>
<li>Anomaly detection algorithms used to identify unusual data points that deviate from the normal patterns in the data</li>
</ul>
<ol start="3" type="1">
<li><strong>Sequential learning</strong>: Sequential Machine Learning (also known as time-series prediction) is a type of machine learning that is focused on making predictions based on sequences of data. It involves training the model on a sequence of inputs, such that the predictions for each time step depend on the previous time steps. Few examples:</li>
</ol>
<ul>
<li>Time series forecasting, used to predict future values based on past trends and patterns in data such as stock prices, weather patterns, and energy consumption</li>
<li>Speech recognition, used to transcribe speech into text by recognizing patterns in audio signals</li>
<li>Natural language processing, used to analyze and make predictions about sequences of text data</li>
</ul>
</section>
</section>
<section id="representation-learning" class="level1">
<h1>Representation Learning</h1>
<p>Representation learning is a sub-field of machine learning that focuses on learning meaningful and compact representations of complex data for tasks such as dimensionality reduction, clustering, and classification.</p>
<p>Given a dataset <span class="math inline">\(\{x_1, x_2, \ldots, x_n\}\)</span> where <span class="math inline">\(x_i \in \mathbb{R}^{d}\)</span>, we need to find a representation for it with the minimum reconstruction error.</p>
<p>Let <span class="math inline">\(w\)</span>, where <span class="math inline">\(||w||=1\)</span>, be the best linear representation of the dataset.</p>
<p>The representation is given by, <span class="math display">\[\begin{align*}
    \frac{(x_i^Tw)}{w^Tw}&amp;w \\
    \text{But }||w||&amp;=1\\
    \therefore \text{ Projection } &amp;= (x_i^Tw)w
\end{align*}\]</span> The reconstruction error is given by, <span class="math display">\[
\text{Reconstruction Error}(f(w)) = \frac{1}{n} \sum _{i=1} ^{n} || x_i - (x_i^Tw)w || ^ 2
\]</span> where <span class="math inline">\(x_i - (x_i^Tw)w\)</span> is the residue and can be given by <span class="math inline">\(x'\)</span>.</p>
<p>Our objective is to minimize the reconstruction error. Minimizing it, we get, <span class="math display">\[\begin{align*}
    \min _{w \in ||w|| = 1} f(w) &amp;= \frac{1}{n} \sum _{i=1} ^{n} -(x_i^Tw)^2 \\
    \therefore \max _{w \in ||w|| = 1} f(w) &amp;= \frac{1}{n} \sum _{i=1} ^{n} (x_i^Tw)^2 \\
    &amp;= w^T(\frac{1}{n} \sum _{i=1} ^{n} x_ix_i^T)w \\
    \max _{w \in ||w|| = 1} f(w) &amp;= w^TCw
\end{align*}\]</span> where <span class="math inline">\(C=\displaystyle \frac{1}{n} \displaystyle \sum _{i=1} ^{n} x_ix_i^T\)</span> is the Covariance Matrix and <span class="math inline">\(C \in \mathbb{R}^{d \times d}\)</span>.</p>
<p>From the above equation we find that <span class="math inline">\(w\)</span> is the eigenvector corresponding to the largest eigenvalue <span class="math inline">\(\lambda\)</span> of <span class="math inline">\(C\)</span>.</p>
<p><span class="math inline">\(w\)</span> is also called the First Principal Component of the dataset.</p>
<section id="potential-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="potential-algorithm">Potential Algorithm</h2>
<p>With this information, we can give the following algorithm:</p>
<p>Given a dataset <span class="math inline">\(\{x_1, x_2, \ldots, x_n\}\)</span> where <span class="math inline">\(x_i \in \mathbb{R}^{d}\)</span>,</p>
<ul>
<li>Center the dataset <span class="math display">\[
\mu = \frac{1}{n} \sum _{i=1} ^{n} x_i
\]</span> <span class="math display">\[
x_i = x_i - \mu  \hspace{2em} \forall i
\]</span></li>
<li>Find the best representation <span class="math inline">\(w \in \mathbb{R}^d\)</span> and <span class="math inline">\(||w|| = 1\)</span>.</li>
<li>Replace <span class="math inline">\(x_i = x_i - (x_i^Tw)w \hspace{1em} \forall i\)</span></li>
<li>Repeat the first three steps until residues become zero and we obtain <span class="math inline">\(w_2, w_3, \ldots, w_d\)</span>.</li>
</ul>
<p>But is this the best way? How many <span class="math inline">\(w\)</span> do we need for optimal compression?</p>
</section>
</section>
<section id="principal-component-analysis" class="level1">
<h1>Principal Component Analysis</h1>
<p>Principal Component Analysis (PCA) is a technique used to reduce the dimensionality of a dataset by identifying the most important features, or principal components, that explain the maximum amount of variance in the data. PCA accomplishes this by transforming the original dataset into a new set of variables that are uncorrelated and ordered by their importance in explaining the variance in the data. This can be helpful for visualizing high-dimensional data or for preprocessing data before performing machine learning tasks.</p>
<p>Following the potential algorithm from above, using the <span class="math inline">\(\{w_1, w_2, \ldots, w_d\}\)</span>, we get, <span class="math display">\[
\forall i \hspace{1em} x_i - ((x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + \ldots +(x_i^Tw_d)w_d) = 0
\]</span> <span class="math display">\[
\therefore x_i = (x_i^Tw_1)w_1 + (x_i^Tw_2)w_2 + \ldots +(x_i^Tw_d)w_d
\]</span> From the above equation, we can say that we can represent the data using <span class="math inline">\(\{x_i^Tw_1, x_i^Tw_2, \ldots, x_i^Tw_d\}\)</span>, which are constants, and <span class="math inline">\(\{w_1, w_2, \ldots, w_d\}\)</span> which are vectors.</p>
<p>Therefore, data which was initially <span class="math inline">\(d \times n\)</span> can now be represented with <span class="math inline">\(d (d + n)\)</span> elements which doesn’t seem great at first.</p>
<p>But if the data lies in a lower sub-space, the residues can be zero without the need for <span class="math inline">\(d\)</span> principal components.</p>
<p>If the data can be represented using only <span class="math inline">\(K\)</span> principal components, where <span class="math inline">\(K &lt;&lt; d\)</span>, the data now can be compressed from <span class="math inline">\(d \times n\)</span> to <span class="math inline">\(k(d + n)\)</span>.</p>
<section id="approximate-representation" class="level2">
<h2 class="anchored" data-anchor-id="approximate-representation">Approximate Representation</h2>
<p>If the data can be approximately represented by a lower sub-space, is it enough that we use only those <span class="math inline">\(K\)</span> projections? How much variance should be covered?</p>
<p>Given a centered dataset <span class="math inline">\(\{x_1, x_2, \ldots, x_n\}\)</span> where <span class="math inline">\(x_i \in \mathbb{R}^{d}\)</span>, let <span class="math inline">\(C\)</span> be its covariance matrix, and let <span class="math inline">\(\{\lambda_1, \lambda_2, \ldots, \lambda_d \}\)</span> be its eigenvalues, which are non-negative because the covariance matrix is a positive semi-definite matrix, placed in descending order, and let <span class="math inline">\(\{w_1, w_2, \ldots, w_d \}\)</span> be its corresponding eigenvectors of unit length.</p>
<p>The eigen equation for the covariance matrix can be given by, <span class="math display">\[\begin{align*}
    Cw &amp;= \lambda w \\
    w^TCw &amp;= w^T\lambda w\\
    \therefore \lambda &amp;= w^TCw \hspace{2em} \{w^Tw = 1\} \\
    \lambda &amp;= \frac{1}{n} \sum _{i=1} ^{n} (x_i^Tw)^2 \\
\end{align*}\]</span> Therefore, as the mean is zero, <span class="math inline">\(\lambda\)</span> gives the variance captured by the eigenvector <span class="math inline">\(w\)</span>.</p>
<p>A good rule of thumb is that the variance captured by P.C.A. should be at least 95%. If the first <span class="math inline">\(K\)</span> eigenvectors capture the required variance, this is given by, <span class="math display">\[
\frac{\displaystyle \sum _{k=1} ^{K} \lambda_k}{\displaystyle \sum _{i=1} ^{d} \lambda_i} \ge 0.95
\]</span> Hence, the higher the variance captured, the lower is the error obtained.</p>
</section>
<section id="p.c.a.-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="p.c.a.-algorithm">P.C.A. Algorithm</h2>
<p>Given a centered dataset <span class="math inline">\(\{x_1, x_2, \ldots, x_n\}\)</span> where <span class="math inline">\(x_i \in \mathbb{R}^{d}\)</span>, let <span class="math inline">\(C\)</span> be its covariance matrix.</p>
<p>The algorithm is as follows:</p>
<ul>
<li>Step 1: Find the eigenvalues and eigenvectors of <span class="math inline">\(C\)</span>. Let <span class="math inline">\(\{\lambda_1, \lambda_2, \ldots, \lambda_d \}\)</span> be its eigenvalues placed in the descending order, and let <span class="math inline">\(\{w_1, w_2, \ldots, w_d \}\)</span> be its corresponding eigenvectors of unit length.</li>
<li>Step 2: Calculate <span class="math inline">\(K\)</span>, where <span class="math inline">\(K\)</span> is the number of required top eigenvalues and eigenvectors, according to the required variance that needs to be covered.</li>
<li>Step 3: Project the data onto the eigenvectors, and obtain the required representation as a linear combination of those projections.</li>
</ul>
<p>In short, we can say that P.C.A. is a dimensionality reduction technique that finds combination of features that are de-correlated (independent of each other).</p>
</section>
</section>
<section id="credits" class="level1">
<h1>Credits</h1>
<ul>
<li>Professor Arun Rajkumar: The content as well as the notations are from his slides and lecture.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>