---
title: "Unsupervised Learning - Representation learning - Kernel PCA"
---

PDF Link: [notes](../notes/Wk02.pdf)

# Introduction
For a given dataset $D \in \mathbb{R} ^{d \times n}$, the covariance matrix is $C \in \mathbb{R} ^{d \times d}$. PCA for this dataset can have the following two problems:

* Time Complexity: The algorithimic complexity of finding the eigenvalues and eigenvectors of $C$ is $O(d ^3)$. Hence, as $d$ grows, the time taken becomes very large.
* Non-Linear Dataset: The dataset may lie in a non-linear subspace. As PCA tries to get linear combination of Principal Components, non-linear datasets may result in non-optimal outputs.

# Reducing the Time Complexity to find Eigenvalues and Eigenvectors
Let's take a dataset $X$ with a large number of features($d$) i.e. $[d >> n]$ where

* $d$: no. of features
* $n$: no. of datapoints

$$
X=\left [
\begin{array}{ccccc}
    | & | & | & & | \\
    x_1 & x_2 & x_3 & \ldots & x_n \\
    | & | & | & & |
\end{array}
\right ]
X \in \mathbb{R} ^{d \times n} 
$$
The covariance matrix $C$ of $X$ is given by,
$$
C=\frac{1}{n}(X  X ^T) \text{\hspace{2em}where } C \in \mathbb{R}^{d \times d} \hspace{2em} \ldots[1]
$$
Let $w_k$ be the the eigenvector corresponding to the $k ^{th}$ largest eigenvalue $\lambda _k$ of $C$.

We know
$$
Cw_k=\lambda _k w_k
$$
Substituting $C$ from $[1]$ in the above equation, and solving for $w_k$, we get
$$
w_k = \sum _{i=1} ^n (\frac{x_i ^T w_k}{n \lambda _k})\cdot{x_i}
$$
$\therefore$ $w_k$ is a linear combination of datapoints! Hence, we can say
$$
w_k= X \alpha _k \text{\hspace{2em} for some } \alpha _k \hspace{2em} \ldots [2]
$$
Let $X ^T X = K$ where $K \in \mathbb{R} ^{n \times n}$. We shall use this to solve for $\alpha _k$. After some algebra (Refer to lectures), we get:
$$
K \alpha _k = (n \lambda _k) \alpha _k
$$
We know that the non-zero eigenvalues of $XX^T$ and $X^T X$ are the same according to the Spectral Theorm.

**Explanation for the above**: If $\lambda \ne 0$ is an eigenvalue of $XX^T$ with eigenvector $w$, then $XX^Tw=\lambda w$.

Multiply both sides by $X^T$ to get $$X^TX(X^Tw)=\lambda X^Tw$$
But then we see that $\lambda$ is still an eigenvalue for $X^TX$ and the corresponding eigenvector is simply $X^Tw$.

Let $\beta _k$ be the the eigenvector corresponding to the $k ^{th}$ largest eigenvalue $n \lambda _k$ of $K$.

On solving the eigen equation for $K$, we get
$$
\alpha _k= \frac{\beta _k}{\sqrt{n \lambda _k}} \hspace{2em} \ldots [3]
$$
Using equations [2] and [3], we can get the eigenvalues and eigenvectors of $C$ using $K$ and hence, bringing the time complexity from $O(d ^3)$ to $O(n^3)$.

# Finding PCA for Non-Linear Relationships
## Transforming Features
We solve the problem of non-linear relationships by mapping them to higher dimensions.
$$
x \to \phi(x) \hspace{2em} \mathbb{R} ^d \to \mathbb{R} ^D \hspace{2em} \text{where } [D >> d]
$$
To compute $D$:

Let $x=\left [
\begin{array} {cc}
    f_1 & f_2
\end{array}
\right ]$ be features of a dataset containing datapoints lying on a curve of degree two in a two-dimensional space.

To make it linear from quadratic, we map the features to 
$\phi(x)=\left [
\begin{array} {cccccc}
    1 & f_1^2 & f_2^2 & f_1f_2 & f_1 & f_2
\end{array}
\right ]$

Mapping $d$ features to the polygonial power $p$ gives $^{d+p} C_d$ new features.

**Issue**: Finding $\phi(x)$ may be very hard.

Solution for this issue is in the next point.

## Kernel Functions
A function that maps $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$, and is a "valid", is called a Kernel Function.

Proof of a "Valid" Kernel:

* Method 1: Exhibit the map to $\phi$ explicitly. [may be hard]
* Method 2: Using Mercer's Theorem:
	+ $k: \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$ is a "valid" kernel if and only if:
		- $k$ is symmetric i.e $k(x,x') = k(x',x)$
		- For any dataset $\{x_1,x_2,\ldots,x_n  \}$, the matrix $K \in \mathbb{R}^{n \times n}$, where $K_{ij} = k(i,j)$, is Positive Semi-Definite.

Two Popular Kernel Functions:

* **Polynomial Kernel**: $k(x,x') = (x^Tx + 1)^p$
* **Radial Basis Function Kernel** or **Gaussian Kernel**: $exp(-\frac{||x-x'||^2}{2\sigma^2})$

# Kernel PCA
Let's take a dataset $X$ with a large number of features($d$) i.e. $[d >> n]$ where 

* $d$: no. of features
* $n$: no. of datapoints
$$
X=\left [
\begin{array}{ccccc}
    | & | & | & & | \\
    x_1 & x_2 & x_3 & \ldots & x_4 \\
    | & | & | & & |
\end{array}
\right ]
$$

* Step 1: Calculate $K \in \mathbb{R}^{n \times n}$ using a kernel function where $K_{ij}=k(x_i,x_j)$.
* Step 2: Center the kernel using the following formula.
    $$
    K^C=K-IK-KI+IKI
    $$
    where $K^C$ is the centered kernel, and $I \in \mathbb{R}^{n \times n}$ where all the elements are $\frac{1}{n}$.
* Step 3: Compute the eigenvalues $\{\beta _1, \beta _2, \ldots, \beta _l\}$ and eigenvectors $\{n\lambda _1, n\lambda _2, \ldots, n\lambda _l\}$ of $K^C$ and normalize to get
    $$
    \forall u \hspace{2em} \alpha _u = \frac{\beta _u}{\sqrt{n \lambda _u}}
    $$
* Step 4: Compute $\sum _{j=1} ^{n} \alpha _{kj} K^C_{ij} \hspace{1em} \forall k$
    $$
    x_i \in \mathbb{R}^{d} \to 
    \left [
\begin{array}{cccc}
    \sum _{j=1} ^{n} \alpha _{1j} K^C_{ij} & \sum _{j=1} ^{n} \alpha _{2j} K^C_{ij} & \ldots & \sum _{j=1} ^{n} \alpha _{dj} K^C_{ij}
\end{array}
\right ]
    $$

# Credits
* Professor Arun Rajkumar: The content as well as the notations are from his slides and lecture.